{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Notes on my problems encountered when trying to implement EM-FDS with Turing's `@model`**\n",
                "\n",
                "1. For some reason not very clear to me, `S` and `T` take on zeros in corresponding entries that result in `NaN`s in `error_rates = T ./ S`. This is why the following loops was necessary.\n",
                "\n",
                "```julia\n",
                "    # ...\n",
                "    error_rates = T ./ S\n",
                "    for a in 1:A, o in 1:O\n",
                "        if hasnans(error_rates[a, o, :])\n",
                "            error_rates[a, o, :] = normalized_class_marginals\n",
                "        end\n",
                "    end\n",
                "    # ...\n",
                "```\n",
                "\n",
                "2. This implementation takes very long to execute, about 2 hours even with relatively small parameters `num_particles = 20`, `num_samples = 20`, `num_iterations = 10`. In comparison, [em-gmm](./em-gmm.ipynb) is quite fast, (although still much slower either than the \"traditional\" `@model` version in [the tutorial](https://turing.ml/dev/tutorials/01-gaussian-mixture-model/) or the EM implementation without `@model`).\n",
                "\n",
                "3. After just 6 iterations with `num_particles = 20`, `num_samples = 20`, the parameters (after passing them through softmax) take on absurd values. One class completely dominates the other (and the effect is even stronger for `num_particles = 50`, `num_samples = 50`)\n",
                "\n",
                "```julia\n",
                "trace = load(\"./trace_2022-09-23T12:54:18.jld\")[\"trace\"]\n",
                "params = map(trace.opt) do o softmax(o.minimizer) end\n",
                "```\n",
                "\n",
                "```sh\n",
                "10-element Vector{Vector{Float64}}:\n",
                " [0.5137429126151726, 0.48625708738482754]\n",
                " [0.5196796341353839, 0.4803203658646161]\n",
                " [0.5551475166649195, 0.44485248333508054]\n",
                " [0.6297826008835576, 0.37021739911644236]\n",
                " [0.888102591461752, 0.1118974085382481]\n",
                " [0.9989157111987694, 0.0010842888012305854]\n",
                " [0.9989157111987694, 0.0010842888012305854]\n",
                " [0.9989157111987694, 0.0010842888012305854]\n",
                " [0.9989157111987694, 0.0010842888012305854]\n",
                " [0.9989157111987694, 0.0010842888012305854]\n",
                "```\n",
                "\n",
                "4. It is possible that equations from [the paper](http://sentic.net/wisdom2018sinha.pdf) (primarily (3) and (4) from page 3) and more generally probabilistic assumptions of the algorithm were not translated correctly.\n",
                "\n",
                "5. If you would like to investigate how model behaves on this dataset with a given choice of sampler parameters before running it, you can load the traces of optimization and chain from `./notebooks/em-gmm/traces`:\n",
                "   - `trace_20p_20s.jld`\n",
                "     - `num_particles = 20; num_samples = 20`\n",
                "   - `trace_50p_50s.jld`\n",
                "     - `num_particles = 50; num_samples = 50`"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# EM-FDS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(800, 164, 2)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "using DawidSkeneAlgorithms\n",
                "import DawidSkeneAlgorithms: initialize_class_assignments, m_step # For `init_param`\n",
                "\n",
                "# Load dataset\n",
                "\n",
                "dataset = load_rte()\n",
                "counts = dataset.x # [question x annotators x classes]\n",
                "size(counts)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "using LinearAlgebra: normalize\n",
                "\n",
                "\"\"\"\n",
                "Initialize parameters.\n",
                "Reuses `m_step` and `initialize_class_assignments` defined for the manual implemention of the algorithm.\n",
                "\"\"\"\n",
                "function init_param(counts)\n",
                "    class_marginals, error_rates = m_step(FDS(), counts, initialize_class_assignments(FDS(), counts))\n",
                "    return normalize(class_marginals[:], 1)\n",
                "end\n",
                "\n",
                "third(x) = x[3]\n",
                "\n",
                "\"\"\"\n",
                "Element-wise multiplication in order to remove annotators that didn't answer the question.\n",
                "\"\"\"\n",
                "function onehot_choices_to_categorical(x::Array{<:Real, 3})::Matrix{Int}\n",
                "    third.(argmax(x; dims=3)[:, :, 1]) .* maximum(x; dims=3)[:, :, 1]\n",
                "end\n",
                "\n",
                "Q, A, O = size(counts) # questions, answers, options\n",
                "choices = onehot_choices_to_categorical(counts)\n",
                "class_marginals = init_param(counts)\n",
                "\n",
                ";"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "em_fds (generic function with 2 methods)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "using Distributions\n",
                "using LogExpFunctions\n",
                "using IterTools\n",
                "using Turing\n",
                "\n",
                "\n",
                "isnanbool(x) = isnan(x) === true\n",
                "hasnans(x::AbstractArray) = isnanbool.(x) |> any\n",
                "allnans(x::AbstractArray) = isnanbool.(x) |> all\n",
                "\n",
                "@model function em_fds(\n",
                "    choices::Matrix{Int}, # [Q x A]\n",
                "    unnormalized_class_marginals::Vector{<:Real} # [O] (probability vector)\n",
                ")\n",
                "    # Get dimensions\n",
                "    Q, A = size(choices) # questions, answers\n",
                "    O = length(unnormalized_class_marginals) # options\n",
                "\n",
                "    # eq (4)\n",
                "    normalized_class_marginals = softmax(unnormalized_class_marginals)\n",
                "    p_Y = Categorical(normalized_class_marginals)\n",
                "    \n",
                "    # sample answer sheet\n",
                "    Ys ~ filldist(p_Y, Q)\n",
                "\n",
                "    # eq (3)\n",
                "\n",
                "    # [A x O]\n",
                "    S = map(product(1:A, 1:O)) do (a, o)\n",
                "        # Sₐ⁽ᶜ⁾ = {i | Yᵢ = c ∧ a has answered quetsion i}\n",
                "        length([q for q in 1:Q \n",
                "                  if Ys[q] == o\n",
                "                  && choices[q, a] ≠ 0])\n",
                "    end\n",
                "\n",
                "    # [A x O x O]\n",
                "    T = map(product(1:A, 1:O, 1:O)) do (a, o_true, o_answered)\n",
                "        # T_{cₐ}⁽ᶜ⁾ = {i | Yᵢ = c ∧ a has answered cₐ on question i}\n",
                "        length([q for q in 1:Q\n",
                "                if Ys[q] == o_true \n",
                "                && choices[q, a] == o_answered])\n",
                "    end\n",
                "\n",
                "    #=\n",
                "    When `NaN`s occur in `error_rates`, they are arranged along the third dimension,\n",
                "    which is sampled as a probability vector.\n",
                "    The minimum risk way of dealing with this is to replace such cases with\n",
                "    priors/`normalized_class_marginals`.\n",
                "    =#\n",
                "    error_rates = T ./ S\n",
                "    for a in 1:A, o in 1:O\n",
                "        if hasnans(error_rates[a, o, :])\n",
                "            error_rates[a, o, :] = normalized_class_marginals\n",
                "        end\n",
                "    end\n",
                "    \n",
                "\n",
                "    #TODO: try moving it into the loop, maybe avoids lines or add an if inside line 62 \n",
                "    p_c_given_Y = [hasnans(error_rates[a, o, :]) ? nothing : Categorical(error_rates[a, o, :]) for a in 1:A, o in 1:O]\n",
                "    for q in 1:Q, a in 1:A\n",
                "        choices[q, a] == 0 && continue\n",
                "        choices[q, a] ~ p_c_given_Y[a, Ys[q]]\n",
                "    end\n",
                "end\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "make_obj (generic function with 1 method)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "function get_latent(chn; keep_pct=.5, use_every=1)\n",
                "    # [latent_dim, num_samples]\n",
                "    chn.value.data[floor(Int, keep_pct * end) : use_every : end, 1:800] |> Matrix{Int} |> transpose\n",
                "end\n",
                "\n",
                "function make_obj(latent)\n",
                "    function obj(unnormalized_class_marginals)\n",
                "        normalized_class_marginals = normalize(unnormalized_class_marginals, 1)\n",
                "        return -mean([\n",
                "            logjoint(em_fds(choices, normalized_class_marginals), (; Ys=Ys))\n",
                "            for Ys in eachcol(latent)\n",
                "            ])\n",
                "    end\n",
                "end"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "i = 1\n",
                        "opt.minimum = 5375.146428809849\n",
                        "chn.logevidence = -4828.24796068154\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "2-element Vector{Float64}:\n",
                            " 0.7021132195158861\n",
                            " 0.4927106311963871"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "i = 2\n",
                        "opt.minimum = 5373.538804173876\n",
                        "chn.logevidence = -4829.785954218567\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "2-element Vector{Float64}:\n",
                            " 0.8807285577246271\n",
                            " 0.5212577017128099"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\r\u001b[32mProgress:  50%|████████████████████▌                    |  ETA: 0:58:21\u001b[39m"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "i = 3\n",
                        "opt.minimum = 5374.868222671934\n",
                        "chn.logevidence = -4837.555133876674\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "2-element Vector{Float64}:\n",
                            " 1.3798581257964482\n",
                            " 0.6552944316104014"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\r\u001b[32mProgress:  75%|██████████████████████████████▊          |  ETA: 0:29:07\u001b[39m"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "i = 4\n",
                        "opt.minimum = 5313.567909955982\n",
                        "chn.logevidence = -4834.967479359389\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "2-element Vector{Float64}:\n",
                            " 2.9746660901293502\n",
                            " 0.1441533556854021"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\r\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 1:56:19\u001b[39m\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Saved as ./em-gmm-traces/trace_50p_50s.jld\n"
                    ]
                }
            ],
            "source": [
                "using Dates\n",
                "using JLD\n",
                "using Random\n",
                "using Optim\n",
                "using ProgressMeter\n",
                "\n",
                "Random.seed!(42)\n",
                "\n",
                "num_particles = 50\n",
                "num_samples = 50\n",
                "num_iterations = 4\n",
                "sampler = PG(num_particles)\n",
                "\n",
                "class_marginals = init_param(counts)\n",
                "trace = (chn=[], opt=[])\n",
                "\n",
                "@showprogress for i in 1:num_iterations\n",
                "    # E-step\n",
                "    chn = sample(em_fds(choices, class_marginals), sampler, num_samples; progress=false)\n",
                "    push!(trace.chn, chn)\n",
                "    \n",
                "    # M-step\n",
                "    opt = optimize(make_obj(get_latent(chn)), class_marginals)\n",
                "    push!(trace.opt, opt)\n",
                "    class_marginals = opt.minimizer\n",
                "\n",
                "    # Report\n",
                "    @show i\n",
                "    @show opt.minimum\n",
                "    @show chn.logevidence\n",
                "    flush(stdout)\n",
                "    display(class_marginals)\n",
                "end\n",
                "\n",
                "savename = \"./em-gmm-traces/trace_$(num_particles)p_$(num_samples)s.jld\"\n",
                "save(savename, \"trace\", trace)\n",
                "println(\"Saved as $savename\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Julia 1.7.3",
            "language": "julia",
            "name": "julia-1.7"
        },
        "language_info": {
            "file_extension": ".jl",
            "mimetype": "application/julia",
            "name": "julia",
            "version": "1.7.3"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}

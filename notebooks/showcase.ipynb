{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem and motivation\n",
    "\n",
    "Whenever we crowd-source annotation of data, we inevitably run into a problem of annotators' fallibility. Having many annotators which disagree with another, how do we extract the true labels from aggregating their annotations?\n",
    "\n",
    "The most straightforward solution is to, for each question (instance to be labeled), take the mode of the votes (the label that annotators give most often).\n",
    "\n",
    "This, however, is not ideal and there are computational approaches that offer better results. The most widely known of those is the Dawid-Skene algorithm ([Dawid and Skene, 1979](https://www.jstor.org/stable/2346806); further: DS).\n",
    "\n",
    "One downside of DS is its relative slowness. [Sinha, Rao, and Balasubramanian (2018)](http://sentic.net/wisdom2018sinha.pdf) propose a faster version of DS called Fast Dawid-Skene which is almost eight times faster than original DS. However, this speedup, comes with a cost of slightly lower accuracy. To balance speed and accuracy, a hybrid version of the algorithm (HDS) is implemented that starts like the normal DS but switches to FDS once the rate of change in the metric falls below a specified threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is to a large extent a Julia version of the [Python implementation of FDS by Sinha, Rao, and Balasubramanian](https://github.com/sukrutrao/Fast-Dawid-Skene).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation Maximization algorithm - high-level description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, FDS has been implemented as an [expectation-maximization (EM) algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm).\n",
    "\n",
    "EM algorithm is a general iterative method for estimating latent variables of statistical models.\n",
    "\n",
    "Given some set of observations, we attempt to discover the parameters of the process that generated these observations. For example if our observations are #### together with labels assigned to them by Mechanical Turk workers, we want to discover the true labels, i.e. to which class a #### actually belongs to. To obtain that information, we need to compute intermediate information, such as *error rates of individual workers* (the probability of each worker giving a wrong label to any datapoint) and *class marginals* (the probability of any question to have a particular true label i.e. frequencies of true labels in the data).\n",
    "\n",
    "EM algorithms achieve their goal by iterating over two steps, called the maximization (M) step and the expectation (E) step. In the M-step, we estimate the values of latent variables, given our current assignment of labels (we choose the values that *maximize* their likelihood given the labels, hence the name of the step). In the E-step, we re-compute the values #CHECKPOINT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To showcase how the three versions of the algorithm work, we will go through the generic version of the algorithm step-by-step. We will note points where DS, FDS, and HDS behave differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "First, we need to load the data. Here, we will use the RTE dataset ([download link](http://ir.ischool.utexas.edu/square/data.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetVoting"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make sure that this notebook is run from the main directory\n",
    "\n",
    "endswith(pwd(), \"/notebooks\") && cd(\"..\")\n",
    "PROJECT_PATH = pwd()\n",
    "\n",
    "include(\"$(PROJECT_PATH)/src/load_datasets.jl\")\n",
    "\n",
    "dataset = load_rte()\n",
    "typeof(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DatasetVoting` struct stores the crowd counts as a 3D `Array` array of shape: `[number_of_questions x number_of_participants x number_of_classes]`.\n",
    "\n",
    "```julia\n",
    "struct DatasetVoting\n",
    "    name::String\n",
    "    crowd_counts::AbstractArray{<:Real, 3}\n",
    "    gold::DataFrame\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typeof(counts) = Array{Float64, 3}\n",
      "size(counts) = (800, 164, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts_values = Dict(0.0 => 254400, 1.0 => 8000)\n"
     ]
    }
   ],
   "source": [
    "using StatsBase: countmap\n",
    "\n",
    "counts = dataset.crowd_counts\n",
    "counts_values = countmap(counts)\n",
    "\n",
    "@show typeof(counts)\n",
    "@show size(counts)\n",
    "@show counts_values\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 800 questions (i.e. instances to be labeled), 164 participants who assigned a label to each question and two possible labels.\n",
    "\n",
    "For example, the `j`-th participant concluded that the `i`-th person should be given label..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 548\n",
      "j = 125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer = [0.0, 0.0]\n",
      "Didn't answer\n"
     ]
    }
   ],
   "source": [
    "using Random\n",
    "\n",
    "n_questions, n_participants = size(counts)\n",
    "i = rand(1:n_questions)\n",
    "j = rand(1:n_participants)\n",
    "answer = counts[i, j, :]\n",
    "\n",
    "@show i\n",
    "@show j\n",
    "@show answer\n",
    "\n",
    "if answer == [1, 0]\n",
    "    println(\"Label 1\")\n",
    "elseif answer == [0, 1]\n",
    "    println(\"Label 2\")\n",
    "else \n",
    "    println(\"Didn't answer\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "[Source code](../src/ExpectationMaximization/src/dawidskene.jl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting algorithms\n",
    "\n",
    "We define three variations of the Dawid-Skene algorithm as `struct`s on which we will dispatch the methods constituting the main `em` method.\n",
    "\n",
    "We also as well as majority voting algorithm to have a baseline to which we can compare their results.\n",
    "\n",
    "First, abstract types.\n",
    "\n",
    "```julia\n",
    "# ExpectationMaximization.jl\n",
    "\n",
    "# Abstract expectation-maximization algorithm type\n",
    "abstract type AbstractEMAlgorithm end\n",
    "\n",
    "# ...\n",
    "\n",
    "abstract type AbstractDawidSkene <: AbstractEMAlgorithm end\n",
    "const ADS = AbstractDawidSkene\n",
    "```\n",
    "\n",
    "Now, concrete type, which we will gather into a vector for convenience.\n",
    "\n",
    "```julia\n",
    "# dawidskene.jl\n",
    "\n",
    "struct FastDawidSkene <: ADS end\n",
    "const FDS = FastDawidSkene\n",
    "\n",
    "struct DawidSkene <: ADS end\n",
    "const DS = DawidSkene\n",
    "\n",
    "struct HybridDawidSkene <: ADS end\n",
    "const HDS = HybridDawidSkene\n",
    "struct HDS_phase2 <: ADS end # This will be explained later\n",
    "\n",
    "struct MajorityVoting <: AbstractEMAlgorithm end \n",
    "const MV = MajorityVoting\n",
    "\n",
    "const VOTING_ALGORITHMS = [FDS(), DS(), HDS(), MV()]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation-maximization algorithm for voting algorithms\n",
    "\n",
    "```julia\n",
    "# dawidskene.jl\n",
    "\n",
    "@enum Verbosity SILENT NORMAL VERBOSE\n",
    "\n",
    "function em(\n",
    "    alg::ADS,\n",
    "    counts::AbstractArray{<:Real, 3};\n",
    "    tol = .0001,\n",
    "    CM_tol = .005,\n",
    "    max_iter = 100,\n",
    "    verbosity::Verbosity = SILENT\n",
    ")\n",
    "# ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization \n",
    "\n",
    "First, we initialize `question_classes` and a few other local variables.\n",
    "\n",
    "```julia\n",
    "# ...\n",
    "    # Matrix of estimates of true classes: [num_of_questions x num_of_responses]\n",
    "    question_classes = initialize_question_classes(alg, counts) \n",
    "    # Number of iterations\n",
    "    nIter = 0\n",
    "    # Whether the algorithm has converged\n",
    "    converged = false\n",
    "    # Prior probabilities (frequencies) of classes\n",
    "    old_class_marginals = nothing\n",
    "    # The probability of participant `k` labeling response `l` for a question whose correct answer is `j` [num_of_participants x num_of_classes x num_of_classes]\n",
    "    old_error_rates = nothing\n",
    "    # Log likelihood of the observations, given current estimates (measure of goodness of fit)\n",
    "    negloglik = nothing\n",
    "```\n",
    "\n",
    "(`initalize_question_classes` is defined in [dawidskene_utilities.jl](../src/ExpectationMaximization/src/dawidskene_utilities.jl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The loop\n",
    "\n",
    "We iterate over the loop until converence. At each iteration, we execute the M-step and the E-step and calculate log likelihood.\n",
    "\n",
    "```julia\n",
    "# ...\n",
    "    while !converged\n",
    "        nIter += 1\n",
    "        class_marginals, error_rates = m_step(alg, counts, question_classes)\n",
    "        question_classes = e_step(alg, counts, class_marginals, error_rates)\n",
    "        negloglik = calculate_negloglikelihood(alg, counts, class_marginals, error_rates)\n",
    "        \n",
    "        # ...\n",
    "    end\n",
    "# ...\n",
    "```\n",
    "\n",
    "(`calculate_negloglikelihood` is defined in [dawidskene_utilities.jl](../src/ExpectationMaximization/src/dawidskene_utilities.jl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M-step\n",
    "\n",
    "In the M-step we use observations (`counts`) and current assignments (`question_classes`) to calculate new estimates of `class_marginals` and annotators' `error_rates`.\n",
    "\n",
    "Note that this step is the same for all versions of the algorithm. However, it requires the algorithm (`::ADS`) to be passed for multiple dispatch since different methods of `m_step` are used by [other versions of the EM algorithm](../src/ExpectationMaximization/src/mixturemodels.jl).\n",
    "\n",
    "```julia\n",
    "function m_step(\n",
    "    ::ADS,\n",
    "    counts::AbstractArray{<:Real, 3},\n",
    "    question_classes::AbstractArray{<:Real, 2}\n",
    ")::Tuple{AbstractArray{<:Real, 2}, AbstractArray{<:Real, 3}} # class_marginals, error_rates\n",
    "\n",
    "    nQuestions, nParticipants, nClasses = size(counts)\n",
    "    class_marginals = sum(question_classes, dims = 1) ./ nQuestions\n",
    "    error_rates = zeros(nParticipants, nClasses, nClasses)\n",
    "    for k in 1:nParticipants\n",
    "        for j in 1:nClasses\n",
    "            for l in 1:nClasses\n",
    "                error_rates[k, j, l] = question_classes[:, j]' * counts[:, k, l]\n",
    "            end\n",
    "            sum_over_responses = sum(error_rates[k, j, :])\n",
    "            if sum_over_responses > 0\n",
    "                error_rates[k, j, :] = error_rates[k, j, :] / sum_over_responses\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return class_marginals, error_rates\n",
    "end\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E-step\n",
    "\n",
    "In the E-step, we again use the observations, along with results of the most recent M-step to compute new assignments.\n",
    "\n",
    "Here, again, there is one method for all versions of the DS. However, it uses a helper function `_e_step_estimate_classes!` that has two different methods that dispatch on the algorithm type. Both return `nothing` and modify `question_classes` or `final_classes` (depending on the algorithm) in-place.\n",
    "\n",
    "```julia\n",
    "function e_step(\n",
    "    alg::ADS,\n",
    "    counts::AbstractArray{<:Real, 3},\n",
    "    class_marginals::AbstractArray{<:Real, 2},\n",
    "    error_rates::AbstractArray{<:Real, 3}\n",
    ")::AbstractArray{<:Real, 2} # question_classes / final_classes\n",
    "\n",
    "    nQuestions, nParticipants, nClasses = size(counts)\n",
    "    question_classes = zeros(nQuestions, nClasses)\n",
    "    final_classes = zeros(nQuestions, nClasses)\n",
    "    for i in 1:nQuestions\n",
    "        for j in 1:nClasses\n",
    "            estimate = class_marginals[j] * prod(error_rates[:, j, :] .^ counts[i, :, :])\n",
    "            question_classes[i, j] = estimate\n",
    "        end\n",
    "        _e_step_estimate_classes!(alg, i, question_classes, final_classes)\n",
    "    end\n",
    "\n",
    "    if typeof(alg) ∈ [DS, HDS]\n",
    "        return question_classes\n",
    "    else # FDS / HDS_phase2\n",
    "        return final_classes\n",
    "    end\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first method dispatches on `DawidSkene` and `HybridDawidSkene`.\n",
    "\n",
    "```julia\n",
    "function _e_step_estimate_classes!(\n",
    "    ::Union{DS, HDS},\n",
    "    i::Int,\n",
    "    question_classes::AbstractArray{<:Real, 2},\n",
    "    final_classes::AbstractArray{<:Real, 2}\n",
    ")::Nothing\n",
    "\n",
    "    question_sum = sum(question_classes[i, :])\n",
    "    if question_sum > 0\n",
    "        question_classes[i, :] = question_classes[i, :] / question_sum\n",
    "    end\n",
    "    return\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be wondering\n",
    "\n",
    "> *If `_e_step_estimate_classes!` dispatches the same method for `DS` and `HDS` and no other function dispatches on the algorithm, then `DS` and `HDS` should work the same way...*\n",
    "\n",
    "The thing is, the other method dispatches on `FDS` **and** `HDS_phase2`\n",
    "\n",
    "```julia\n",
    "function _e_step_estimate_classes!(\n",
    "    ::Union{FDS, HDS_phase2},\n",
    "    i::Int,\n",
    "    question_classes::AbstractArray{<:Real, 2},\n",
    "    final_classes::AbstractArray{<:Real, 2}\n",
    ")::Nothing\n",
    "\n",
    "    maxval = maximum(question_classes[i, :])\n",
    "    maxinds = argwhere(question_classes[i, :], ==(maxval))\n",
    "    final_classes[i, sample(maxinds, 1)[1]] = 1\n",
    "    return\n",
    "end\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we change `alg` from `HDS` to `HDS_phase2` after the rate of change of `class_marginals` slows down below the threshold value of `CM_tol` which we passed as an argument to `em`.\n",
    "\n",
    "Another argument `tol` specifies the rate of change of class marginals below which we decide that the algorithm has converged. The convergence can also occur if we have exceeded a specified maximum number of iterations.\n",
    "\n",
    "```julia\n",
    "# ...\n",
    "    while !converged \n",
    "        # ...\n",
    "\n",
    "        if old_class_marginals ≠ nothing\n",
    "            # How much have `class_marginals` and `error_rates`\n",
    "            #  changed since last iteration\n",
    "            class_marginals_diff = sum(abs.(class_marginals - old_class_marginals))\n",
    "            error_rates_diff = sum(abs.(error_rates - old_error_rates))\n",
    "            # If class marginals change rate goes below a certain threshold \n",
    "            #  or we have done too many iterations consider the algorithm converged\n",
    "            if class_marginals_diff < tol || nIter >= max_iter\n",
    "                converged = true\n",
    "            # If using Hybrid Dawid-Skene and class marginals change rate\n",
    "            #  goes below the threshold, switch the algorithm to phase 2,\n",
    "            #  in which it works like Fast Dawid-Skene\n",
    "            elseif alg isa HDS && class_marginals_diff ≤ CM_tol\n",
    "                alg = HDS_phase2()\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        old_class_marginals = class_marginals\n",
    "        old_error_rates = error_rates\n",
    "\n",
    "        if verbosity == VERBOSE\n",
    "            @show nIter\n",
    "            @show negloglik\n",
    "        end\n",
    "    end\n",
    "# ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in short, Hybrid Dawid-Skene works just like normal Dawid-Skene *until* the rate of change of estimated class marginals decreases below a certain threshold. From that point onward, it works like Fast Dawid-Skene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "After convergence, we have the array `question_classes` (`[num_of_questions x num_of_classes]`) which stores the probability distribution over the classes for each question.\n",
    "\n",
    "We take the most probable class for each question and return it along with the log likelihood which was calculated after each pair of M- and E-steps.\n",
    "\n",
    "```julia\n",
    "    verbosity == NORMAL && @show negloglik\n",
    "    result = map(\n",
    "        x -> x[2], \n",
    "        argmax(question_classes, dims = 2)[:])\n",
    "    return result, negloglik\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run\n",
    "\n",
    "Let's load everything and run the algorithms on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{ExpectationMaximization.AbstractEMAlgorithm}:\n",
       " FastDawidSkene\n",
       " DawidSkene\n",
       " HybridDawidSkene\n",
       " MajorityVoting"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "using ExpectationMaximization: FDS, DS, HDS, MV\n",
    "\n",
    "VOTING_ALGORITHMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Dawid-Skene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  46.490 ms (71644 allocations: 135.61 MiB)\n"
     ]
    }
   ],
   "source": [
    "result, negloglikelihood = @btime em($FDS(), $dataset.crowd_counts) seconds=5\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast Dawid-Skene\n",
      "Negative log-likelihood: 3747.47\n"
     ]
    }
   ],
   "source": [
    "println(\"Fast Dawid-Skene\n",
    "Negative log-likelihood: $(round(negloglikelihood; digits=2))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dawid-Skene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  125.755 ms (159787 allocations: 370.52 MiB)\n"
     ]
    }
   ],
   "source": [
    "result, negloglikelihood = @btime em($DS(), $dataset.crowd_counts) seconds=5\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dawid-Skene\n",
      "Negative log-likelihood: 3679.63\n"
     ]
    }
   ],
   "source": [
    "println(\"Dawid-Skene\n",
    "Negative log-likelihood: $(round(negloglikelihood; digits=2))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Dawid-Skene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  101.595 ms (138375 allocations: 303.63 MiB)\n"
     ]
    }
   ],
   "source": [
    "result, negloglikelihood = @btime em($HDS(), $dataset.crowd_counts) seconds=5\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Dawid-Skene\n",
      "Negative log-likelihood: 3680.32\n"
     ]
    }
   ],
   "source": [
    "println(\"Hybrid Dawid-Skene\n",
    "Negative log-likelihood: $(round(negloglikelihood; digits=2))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Majority Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5.611 ms (11918 allocations: 21.22 MiB)\n"
     ]
    }
   ],
   "source": [
    "result, negloglikelihood = @btime em($MV(), $dataset.crowd_counts) seconds=5\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Voting\n",
      "Negative log-likelihood: 3782.4\n"
     ]
    }
   ],
   "source": [
    "println(\"Majority Voting\n",
    "Negative log-likelihood: $(round(negloglikelihood; digits=2))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, Hybrid Dawid-Skene is a little bit closer to Fast Dawid-Skene in speed but achieves log-likelihood comparable to normal Dawid-Skene.\n",
    "\n",
    "What's important, these values are very close to those reported in [the paper](http://sentic.net/wisdom2018sinha.pdf) for the RTE dataset (Table 2) which strongly suggests that our implementation works as intended.\n",
    "\n",
    "![](../images/FDS%20Paper%20-%20Table2.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
